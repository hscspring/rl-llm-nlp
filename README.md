# RL-LLM-NLP
This repository encompasses libraries and papers on Reinforcement Learning (RL) within Large Language Models (LLM) and Natural Language Processing (NLP).

I consider RL to be a pivotal technology in the field of AI, and NLP (particularly LLM) to be a direction well worth exploring.

## Library

| GitHub                                                       | From        | Year | Desc                                                         |
| ------------------------------------------------------------ | ----------- | ---- | ------------------------------------------------------------ |
| [PRIME](https://github.com/PRIME-RL/PRIME)                   | PRIME-RL    | 2025 | Scalable RL solution for the advanced reasoning of language models |
| [vlRL](https://github.com/volcengine/verl)                   | Bytedance   | 2024 | Volcano Engine Reinforcement Learning for LLM                |
| [trl](https://github.com/huggingface/trl)                    | HuggingFace | 2024 | Train LM with RL                                             |
| [RL4LMs](https://github.com/allenai/RL4LMs)                  | Allen       | 2023 | RL library to fine-tune LM to human preferences              |
| [alignment-handbook](https://github.com/huggingface/alignment-handbook) | huggingface | 2023 | Robust recipes to align language models with human and AI preferences |

## Paper

| Title                                                        | From        | Year | Link                                                         |
| ------------------------------------------------------------ | ----------- | ---- | ------------------------------------------------------------ |
| Process Reinforcement through Implicit Rewards               |             | 2025 | [paper](https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f), [GitHub](https://github.com/PRIME-RL/PRIME) |
| rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers | MicroSoft   | 2024 | [paper](https://arxiv.org/pdf/2408.06195), [GitHub](https://github.com/zhentingqi/rStar) |
| A Closer Look at Machine Unlearning for Large Language Models | Sea AI      | 2024 | [paper](https://arxiv.org/abs/2410.08109v1), [GitHub](https://github.com/sail-sg/closer-look-LLM-unlearning) |
| Direct Preference Optimization: Your Language Model is Secretly a Reward Model | Stanford    | 2024 | [paper](https://arxiv.org/abs/2305.18290)                    |
| A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More | Salesforce  | 2024 | [paper](https://arxiv.org/abs/2407.16216)                    |
| Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback | Allen       | 2024 | [paper](https://arxiv.org/abs/2406.09279), [GitHub](https://github.com/hamishivi/EasyLM) |
| Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey | Capital One | 2024 | [paper](http://arxiv.org/abs/2409.11564)                     |
| IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION | Allen       | 2022 | [paper](http://arxiv.org/abs/2210.01241), [GitHub](https://github.com/allenai/rl4lms) |
| Quark: Controllable Text Generation with Reinforced [Un]learning | Allen       | 2022 | [paper](http://arxiv.org/abs/2205.13636), [GitHub](https://github.com/GXimingLu/Quark) |
| Decision Transformer: Reinforcement Learning via Sequence Modeling | Berkeley    | 2021 | [paper](https://arxiv.org/abs/2106.01345), [GitHub](https://github.com/kzl/decision-transformer) |
| Fine-Tuning Language Models from Human Preferences           | OpenAI      | 2020 | [paper](http://arxiv.org/abs/1909.08593), [GitHub](https://github.com/openai/lm-human-preferences) |

