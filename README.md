# RL-LLM-NLP
This repository encompasses libraries and papers on Reinforcement Learning (RL) within Large Language Models (LLM) and Natural Language Processing (NLP).

I consider RL to be a pivotal technology in the field of AI, and NLP (particularly LLM) to be a direction well worth exploring.

## Library

| GitHub                                      | From        | Year | Desc                                            |
| ------------------------------------------- | ----------- | ---- | ----------------------------------------------- |
| [trl](https://github.com/huggingface/trl)   | HuggingFace | 2024 | Train LM with RL                                |
| [RL4LMs](https://github.com/allenai/RL4LMs) | Allen       | 2023 | RL library to fine-tune LM to human preferences |

## Paper

| Title                                                        | From        | Year | Link                                                         |
| ------------------------------------------------------------ | ----------- | ---- | ------------------------------------------------------------ |
| A Closer Look at Machine Unlearning for Large Language Models | Sea AI      | 2024 | [paper](https://arxiv.org/abs/2410.08109v1), [GitHub](https://github.com/sail-sg/closer-look-LLM-unlearning) |
| Direct Preference Optimization: Your Language Model is Secretly a Reward Model | Stanford    | 2024 | [paper](https://arxiv.org/abs/2305.18290)                    |
| A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More | Salesforce  | 2024 | [paper](https://arxiv.org/abs/2407.16216)                    |
| Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback | Allen       | 2024 | [paper](https://arxiv.org/abs/2406.09279), [GitHub](https://github.com/hamishivi/EasyLM) |
| Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey | Capital One | 2024 | [paper](http://arxiv.org/abs/2409.11564)                     |
| IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION | Allen       | 2022 | [paper](http://arxiv.org/abs/2210.01241), [GitHub](https://github.com/allenai/rl4lms) |
| Quark: Controllable Text Generation with Reinforced [Un]learning | Allen       | 2022 | [paper](http://arxiv.org/abs/2205.13636), [GitHub](https://github.com/GXimingLu/Quark) |
| Decision Transformer: Reinforcement Learning via Sequence Modeling | Berkeley    | 2021 | [paper](https://arxiv.org/abs/2106.01345), [GitHub](https://github.com/kzl/decision-transformer) |
| Fine-Tuning Language Models from Human Preferences           | OpenAI      | 2020 | [paper](http://arxiv.org/abs/1909.08593), [GitHub](https://github.com/openai/lm-human-preferences) |

